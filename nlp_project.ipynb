{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "#validation = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum nuber of words that can be present in a comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].apply(lambda x:len(str(x).split())).max()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function for getting auc score for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    #f1_score = metrics.f1_score(target, predictions ) #average=\"micro\" weighted macro\n",
    "    #accuracy = metrics.accuracy_score(target, predictions)\n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, train.target.values, \n",
    "                                                  stratify=train.target.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project im going to use RNN as my model\n",
    "Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer. \n",
    "\n",
    "A recurrent newral network can be thought of as multiple copies of the same network, each passing a message to a successor. This type of flow of information through time (or sequence) in a recurrent neural network is shown in the diagram below, which unrolls the sequence (loop unrolled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://miro.medium.com/max/720/1%2As1EGNG54_4j93SCQF-NvKA.webp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This unrolled network shows how we can supply a stream of data (intimately related to sequences, lists and time-series data) to the recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Tokenization\n",
    "\n",
    "In RNN we input a sentence word by word. We represent every word as one hot vectors of dimensions : Numbers of words in Vocab +1.\n",
    "What keras Tokenizer does is , it takes all the unique words in the corpus,forms a dictionary with words as keys and their number of occurences as values,it then sorts the dictionary in descending order of counts. It then assigns the first value 1 , second value 2 and so on. So let's suppose word 'the' occured the most in the corpus then it will assigned index 1 and vector representing 'the' would be a one-hot vector with value 1 at position 1 and rest zereos.\n",
    "Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 50\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 1,\n",
       " 'co': 2,\n",
       " 'http': 3,\n",
       " 'the': 4,\n",
       " 'a': 5,\n",
       " 'in': 6,\n",
       " 'to': 7,\n",
       " 'of': 8,\n",
       " 'and': 9,\n",
       " 'i': 10,\n",
       " 'is': 11,\n",
       " 'for': 12,\n",
       " 'on': 13,\n",
       " 'you': 14,\n",
       " 'my': 15,\n",
       " 'with': 16,\n",
       " 'that': 17,\n",
       " 'it': 18,\n",
       " 'at': 19,\n",
       " 'by': 20,\n",
       " 'this': 21,\n",
       " 'from': 22,\n",
       " 'https': 23,\n",
       " 'are': 24,\n",
       " 'be': 25,\n",
       " 'was': 26,\n",
       " 'have': 27,\n",
       " 'like': 28,\n",
       " 'amp': 29,\n",
       " 'as': 30,\n",
       " 'up': 31,\n",
       " 'me': 32,\n",
       " 'but': 33,\n",
       " 'just': 34,\n",
       " 'so': 35,\n",
       " 'not': 36,\n",
       " 'your': 37,\n",
       " 'out': 38,\n",
       " 'no': 39,\n",
       " 'all': 40,\n",
       " 'after': 41,\n",
       " 'will': 42,\n",
       " 'has': 43,\n",
       " 'an': 44,\n",
       " 'fire': 45,\n",
       " 'when': 46,\n",
       " \"i'm\": 47,\n",
       " 'if': 48,\n",
       " 'get': 49,\n",
       " 'we': 50,\n",
       " 'now': 51,\n",
       " 'new': 52,\n",
       " 'via': 53,\n",
       " 'more': 54,\n",
       " '2': 55,\n",
       " 'about': 56,\n",
       " 'or': 57,\n",
       " 'news': 58,\n",
       " 'people': 59,\n",
       " 'what': 60,\n",
       " 'he': 61,\n",
       " 'one': 62,\n",
       " 'they': 63,\n",
       " 'how': 64,\n",
       " 'been': 65,\n",
       " 'over': 66,\n",
       " 'who': 67,\n",
       " \"it's\": 68,\n",
       " 'into': 69,\n",
       " \"don't\": 70,\n",
       " 'do': 71,\n",
       " 'video': 72,\n",
       " \"'\": 73,\n",
       " 'can': 74,\n",
       " 'emergency': 75,\n",
       " 'disaster': 76,\n",
       " 'there': 77,\n",
       " 'police': 78,\n",
       " 'than': 79,\n",
       " '3': 80,\n",
       " 'her': 81,\n",
       " 'u': 82,\n",
       " 'would': 83,\n",
       " 'his': 84,\n",
       " 'still': 85,\n",
       " 'some': 86,\n",
       " 'body': 87,\n",
       " 'were': 88,\n",
       " 'us': 89,\n",
       " 'burning': 90,\n",
       " 'back': 91,\n",
       " 'storm': 92,\n",
       " 'crash': 93,\n",
       " 'day': 94,\n",
       " 'time': 95,\n",
       " 'them': 96,\n",
       " 'california': 97,\n",
       " '1': 98,\n",
       " 'off': 99,\n",
       " 'got': 100,\n",
       " 'know': 101,\n",
       " 'why': 102,\n",
       " 'suicide': 103,\n",
       " 'man': 104,\n",
       " 'had': 105,\n",
       " 'rt': 106,\n",
       " 's': 107,\n",
       " 'buildings': 108,\n",
       " 'first': 109,\n",
       " 'see': 110,\n",
       " 'going': 111,\n",
       " 'world': 112,\n",
       " 'bomb': 113,\n",
       " 'fires': 114,\n",
       " 'nuclear': 115,\n",
       " 'two': 116,\n",
       " 'love': 117,\n",
       " 'killed': 118,\n",
       " 'our': 119,\n",
       " 'year': 120,\n",
       " 'youtube': 121,\n",
       " 'attack': 122,\n",
       " 'dead': 123,\n",
       " 'their': 124,\n",
       " 'gt': 125,\n",
       " 'train': 126,\n",
       " 'full': 127,\n",
       " '4': 128,\n",
       " 'go': 129,\n",
       " 'life': 130,\n",
       " 'car': 131,\n",
       " '5': 132,\n",
       " 'old': 133,\n",
       " 'war': 134,\n",
       " 'hiroshima': 135,\n",
       " 'being': 136,\n",
       " 'today': 137,\n",
       " 'only': 138,\n",
       " 'accident': 139,\n",
       " 'families': 140,\n",
       " 'down': 141,\n",
       " 'may': 142,\n",
       " \"can't\": 143,\n",
       " 'good': 144,\n",
       " 'think': 145,\n",
       " 'here': 146,\n",
       " '2015': 147,\n",
       " 'watch': 148,\n",
       " 'say': 149,\n",
       " 'last': 150,\n",
       " 'many': 151,\n",
       " 'did': 152,\n",
       " 'home': 153,\n",
       " 'way': 154,\n",
       " 'could': 155,\n",
       " 'its': 156,\n",
       " 'years': 157,\n",
       " 'too': 158,\n",
       " 'then': 159,\n",
       " 'want': 160,\n",
       " 'w': 161,\n",
       " 'make': 162,\n",
       " 'collapse': 163,\n",
       " 'work': 164,\n",
       " 'because': 165,\n",
       " 'best': 166,\n",
       " 'look': 167,\n",
       " 'even': 168,\n",
       " 'mass': 169,\n",
       " 'another': 170,\n",
       " 'help': 171,\n",
       " 'please': 172,\n",
       " 'need': 173,\n",
       " 'take': 174,\n",
       " 'mh370': 175,\n",
       " 'am': 176,\n",
       " 'army': 177,\n",
       " 'really': 178,\n",
       " 'wildfire': 179,\n",
       " 'death': 180,\n",
       " 'lol': 181,\n",
       " 'bombing': 182,\n",
       " 'him': 183,\n",
       " 'should': 184,\n",
       " 'black': 185,\n",
       " 'right': 186,\n",
       " 'those': 187,\n",
       " 'school': 188,\n",
       " 'forest': 189,\n",
       " 'hot': 190,\n",
       " 'fatal': 191,\n",
       " \"you're\": 192,\n",
       " '\\x89û': 193,\n",
       " '8': 194,\n",
       " 'pm': 195,\n",
       " 'much': 196,\n",
       " '11': 197,\n",
       " 'northern': 198,\n",
       " 'obama': 199,\n",
       " 'live': 200,\n",
       " 'let': 201,\n",
       " 'never': 202,\n",
       " 'city': 203,\n",
       " '9': 204,\n",
       " 'she': 205,\n",
       " 'bomber': 206,\n",
       " 'great': 207,\n",
       " 'wreck': 208,\n",
       " 'latest': 209,\n",
       " 'homes': 210,\n",
       " 'any': 211,\n",
       " '6': 212,\n",
       " 'every': 213,\n",
       " 'typhoon': 214,\n",
       " 'read': 215,\n",
       " 'atomic': 216,\n",
       " 'god': 217,\n",
       " 'fear': 218,\n",
       " 'said': 219,\n",
       " 'flood': 220,\n",
       " 'flames': 221,\n",
       " 'where': 222,\n",
       " 'floods': 223,\n",
       " 'ever': 224,\n",
       " 'injured': 225,\n",
       " 'shit': 226,\n",
       " 'under': 227,\n",
       " 'im': 228,\n",
       " 'come': 229,\n",
       " 'getting': 230,\n",
       " 'top': 231,\n",
       " 'near': 232,\n",
       " 'japan': 233,\n",
       " 'damage': 234,\n",
       " 'feel': 235,\n",
       " 'while': 236,\n",
       " 'most': 237,\n",
       " 'since': 238,\n",
       " 'cross': 239,\n",
       " \"that's\": 240,\n",
       " 'hit': 241,\n",
       " 'oil': 242,\n",
       " 'hope': 243,\n",
       " 'everyone': 244,\n",
       " 'found': 245,\n",
       " '15': 246,\n",
       " 'content': 247,\n",
       " 'before': 248,\n",
       " 'military': 249,\n",
       " 'ass': 250,\n",
       " '10': 251,\n",
       " 'weather': 252,\n",
       " 'during': 253,\n",
       " 'stop': 254,\n",
       " 'coming': 255,\n",
       " 'debris': 256,\n",
       " 'evacuation': 257,\n",
       " 'wild': 258,\n",
       " 'these': 259,\n",
       " 'next': 260,\n",
       " 'night': 261,\n",
       " 'truck': 262,\n",
       " 'malaysia': 263,\n",
       " 'which': 264,\n",
       " 'state': 265,\n",
       " 'without': 266,\n",
       " 'times': 267,\n",
       " 'flooding': 268,\n",
       " 'earthquake': 269,\n",
       " 'plan': 270,\n",
       " 'smoke': 271,\n",
       " 'lightning': 272,\n",
       " 'thunderstorm': 273,\n",
       " '08': 274,\n",
       " 'set': 275,\n",
       " 'movie': 276,\n",
       " 'well': 277,\n",
       " 'face': 278,\n",
       " 'm': 279,\n",
       " 'severe': 280,\n",
       " 'water': 281,\n",
       " 'check': 282,\n",
       " 'through': 283,\n",
       " 'area': 284,\n",
       " 'heat': 285,\n",
       " 'looks': 286,\n",
       " 'little': 287,\n",
       " 'explosion': 288,\n",
       " 'made': 289,\n",
       " 'fucking': 290,\n",
       " 'run': 291,\n",
       " 'wounded': 292,\n",
       " 'always': 293,\n",
       " 'sinking': 294,\n",
       " 'says': 295,\n",
       " 'natural': 296,\n",
       " '7': 297,\n",
       " 'services': 298,\n",
       " 'warning': 299,\n",
       " 'cause': 300,\n",
       " 'high': 301,\n",
       " 'thunder': 302,\n",
       " 'bad': 303,\n",
       " 'rain': 304,\n",
       " '70': 305,\n",
       " 'loud': 306,\n",
       " 'liked': 307,\n",
       " 'change': 308,\n",
       " 'bloody': 309,\n",
       " 'until': 310,\n",
       " 'hurricane': 311,\n",
       " 'injuries': 312,\n",
       " 'fall': 313,\n",
       " 'also': 314,\n",
       " 'hail': 315,\n",
       " 'devastated': 316,\n",
       " 'spill': 317,\n",
       " 'reddit': 318,\n",
       " 'family': 319,\n",
       " 'again': 320,\n",
       " 'gonna': 321,\n",
       " 'evacuate': 322,\n",
       " 'head': 323,\n",
       " 'weapon': 324,\n",
       " 'injury': 325,\n",
       " 'weapons': 326,\n",
       " 'house': 327,\n",
       " 'free': 328,\n",
       " 'photo': 329,\n",
       " 'refugees': 330,\n",
       " 'red': 331,\n",
       " 'end': 332,\n",
       " 'missing': 333,\n",
       " 'wind': 334,\n",
       " 'rescue': 335,\n",
       " 'blood': 336,\n",
       " 'bags': 337,\n",
       " 'collided': 338,\n",
       " 'summer': 339,\n",
       " 'screaming': 340,\n",
       " 'fatalities': 341,\n",
       " 'p': 342,\n",
       " 'trapped': 343,\n",
       " 'air': 344,\n",
       " 'girl': 345,\n",
       " 'sinkhole': 346,\n",
       " 'explode': 347,\n",
       " 'failure': 348,\n",
       " 'big': 349,\n",
       " 'whole': 350,\n",
       " 'murder': 351,\n",
       " 'panic': 352,\n",
       " 'collision': 353,\n",
       " 'destroy': 354,\n",
       " 'released': 355,\n",
       " 'attacked': 356,\n",
       " 'derailment': 357,\n",
       " \"he's\": 358,\n",
       " 'bridge': 359,\n",
       " 'breaking': 360,\n",
       " 'destroyed': 361,\n",
       " 'destruction': 362,\n",
       " '40': 363,\n",
       " 'outbreak': 364,\n",
       " 'harm': 365,\n",
       " 'saudi': 366,\n",
       " 'terrorism': 367,\n",
       " 'hazard': 368,\n",
       " 'boy': 369,\n",
       " 'real': 370,\n",
       " 'around': 371,\n",
       " 'someone': 372,\n",
       " 'post': 373,\n",
       " 'bag': 374,\n",
       " 'ambulance': 375,\n",
       " \"i've\": 376,\n",
       " 'wreckage': 377,\n",
       " 'game': 378,\n",
       " 'burned': 379,\n",
       " 'drought': 380,\n",
       " 'report': 381,\n",
       " 'week': 382,\n",
       " 'road': 383,\n",
       " 'keep': 384,\n",
       " 'survivors': 385,\n",
       " '\\x89ûò': 386,\n",
       " 'survive': 387,\n",
       " 'migrants': 388,\n",
       " 'charged': 389,\n",
       " 'twister': 390,\n",
       " '05': 391,\n",
       " 'update': 392,\n",
       " 'deaths': 393,\n",
       " 'wrecked': 394,\n",
       " 'terrorist': 395,\n",
       " 'does': 396,\n",
       " 'trauma': 397,\n",
       " 'battle': 398,\n",
       " 'service': 399,\n",
       " 'away': 400,\n",
       " 'rescued': 401,\n",
       " 'ruin': 402,\n",
       " 'fuck': 403,\n",
       " 'self': 404,\n",
       " 'county': 405,\n",
       " 'ok': 406,\n",
       " 'call': 407,\n",
       " 'rescuers': 408,\n",
       " 'boat': 409,\n",
       " 'tonight': 410,\n",
       " 'windstorm': 411,\n",
       " 'crush': 412,\n",
       " 'dust': 413,\n",
       " 'n': 414,\n",
       " 'bombed': 415,\n",
       " 'white': 416,\n",
       " 'survived': 417,\n",
       " 'd': 418,\n",
       " 'against': 419,\n",
       " 'o': 420,\n",
       " 'displaced': 421,\n",
       " 'whirlwind': 422,\n",
       " 'hazardous': 423,\n",
       " 'twitter': 424,\n",
       " 'danger': 425,\n",
       " 'structural': 426,\n",
       " 'bus': 427,\n",
       " 'riot': 428,\n",
       " 'crashed': 429,\n",
       " '0': 430,\n",
       " 'curfew': 431,\n",
       " 'show': 432,\n",
       " 'story': 433,\n",
       " 'put': 434,\n",
       " '30': 435,\n",
       " 'lives': 436,\n",
       " \"there's\": 437,\n",
       " 'massacre': 438,\n",
       " 'things': 439,\n",
       " 'august': 440,\n",
       " 'island': 441,\n",
       " 'half': 442,\n",
       " 'screamed': 443,\n",
       " 'chemical': 444,\n",
       " 'least': 445,\n",
       " 'deluge': 446,\n",
       " 'landslide': 447,\n",
       " 'famine': 448,\n",
       " 'other': 449,\n",
       " 'investigators': 450,\n",
       " 'tragedy': 451,\n",
       " 'saw': 452,\n",
       " 'phone': 453,\n",
       " 'derailed': 454,\n",
       " 'hostage': 455,\n",
       " 'came': 456,\n",
       " 'sandstorm': 457,\n",
       " 'violent': 458,\n",
       " 'suspect': 459,\n",
       " 'hostages': 460,\n",
       " 'catastrophe': 461,\n",
       " 'collapsed': 462,\n",
       " 'long': 463,\n",
       " 'sunk': 464,\n",
       " 'devastation': 465,\n",
       " 'wanna': 466,\n",
       " 'traumatised': 467,\n",
       " 'quarantined': 468,\n",
       " 'blown': 469,\n",
       " 'casualties': 470,\n",
       " 'past': 471,\n",
       " 'kills': 472,\n",
       " 'heard': 473,\n",
       " 'bang': 474,\n",
       " 'airplane': 475,\n",
       " 'derail': 476,\n",
       " 'screams': 477,\n",
       " 'inundated': 478,\n",
       " 'stock': 479,\n",
       " 'rioting': 480,\n",
       " 'woman': 481,\n",
       " 'better': 482,\n",
       " 'engulfed': 483,\n",
       " 'tornado': 484,\n",
       " 'part': 485,\n",
       " 'bleeding': 486,\n",
       " 'heart': 487,\n",
       " 'national': 488,\n",
       " 'apocalypse': 489,\n",
       " 'power': 490,\n",
       " 'wave': 491,\n",
       " 'drowning': 492,\n",
       " 'thing': 493,\n",
       " 'oh': 494,\n",
       " 'anniversary': 495,\n",
       " 'quarantine': 496,\n",
       " 'women': 497,\n",
       " 'horrible': 498,\n",
       " 'desolation': 499,\n",
       " 'plane': 500,\n",
       " 'light': 501,\n",
       " \"i'll\": 502,\n",
       " 'lava': 503,\n",
       " 'catastrophic': 504,\n",
       " 'exploded': 505,\n",
       " 'wounds': 506,\n",
       " 'went': 507,\n",
       " 'flattened': 508,\n",
       " 'fatality': 509,\n",
       " 'hundreds': 510,\n",
       " 'cliff': 511,\n",
       " 'iran': 512,\n",
       " 'save': 513,\n",
       " 'electrocuted': 514,\n",
       " 'fedex': 515,\n",
       " 'group': 516,\n",
       " 'meltdown': 517,\n",
       " 'b': 518,\n",
       " 'armageddon': 519,\n",
       " 'bagging': 520,\n",
       " 'trouble': 521,\n",
       " 'panicking': 522,\n",
       " 'soon': 523,\n",
       " 'pandemonium': 524,\n",
       " 'caused': 525,\n",
       " 'hijacker': 526,\n",
       " 'affected': 527,\n",
       " 'food': 528,\n",
       " 'mosque': 529,\n",
       " 'ebay': 530,\n",
       " 'evacuated': 531,\n",
       " 'must': 532,\n",
       " 'use': 533,\n",
       " 'thank': 534,\n",
       " 'something': 535,\n",
       " 'blew': 536,\n",
       " 'drown': 537,\n",
       " '00': 538,\n",
       " 'land': 539,\n",
       " 'st': 540,\n",
       " \"'the\": 541,\n",
       " 'lot': 542,\n",
       " 'razed': 543,\n",
       " 'tsunami': 544,\n",
       " 'zone': 545,\n",
       " 'hellfire': 546,\n",
       " 'reunion': 547,\n",
       " 'rainstorm': 548,\n",
       " 'very': 549,\n",
       " \"legionnaires'\": 550,\n",
       " 'cool': 551,\n",
       " 'market': 552,\n",
       " 'baby': 553,\n",
       " 'minute': 554,\n",
       " 'left': 555,\n",
       " 'send': 556,\n",
       " 'river': 557,\n",
       " 'bioterror': 558,\n",
       " 'sure': 559,\n",
       " 'calgary': 560,\n",
       " 'hijacking': 561,\n",
       " 'blazing': 562,\n",
       " 'detonation': 563,\n",
       " 'india': 564,\n",
       " 'tomorrow': 565,\n",
       " 'casualty': 566,\n",
       " 'government': 567,\n",
       " 'demolish': 568,\n",
       " 'security': 569,\n",
       " 'goes': 570,\n",
       " 'possible': 571,\n",
       " 'isis': 572,\n",
       " 'song': 573,\n",
       " 'e': 574,\n",
       " 'drowned': 575,\n",
       " 'due': 576,\n",
       " 'crushed': 577,\n",
       " 'murderer': 578,\n",
       " 'care': 579,\n",
       " 'thought': 580,\n",
       " 'collide': 581,\n",
       " 'longer': 582,\n",
       " 'volcano': 583,\n",
       " 'doing': 584,\n",
       " 'obliterated': 585,\n",
       " 'traffic': 586,\n",
       " 'south': 587,\n",
       " 'demolition': 588,\n",
       " 'r': 589,\n",
       " 'officials': 590,\n",
       " 'kill': 591,\n",
       " 'pkk': 592,\n",
       " 'detonated': 593,\n",
       " 'obliterate': 594,\n",
       " 'fan': 595,\n",
       " 'three': 596,\n",
       " 'blast': 597,\n",
       " 'building': 598,\n",
       " 'mudslide': 599,\n",
       " 'stay': 600,\n",
       " 'annihilated': 601,\n",
       " 'issues': 602,\n",
       " 'demolished': 603,\n",
       " 'beautiful': 604,\n",
       " 'used': 605,\n",
       " 'kids': 606,\n",
       " 'airport': 607,\n",
       " 'same': 608,\n",
       " 'officer': 609,\n",
       " 'c': 610,\n",
       " 'prebreak': 611,\n",
       " 'shoulder': 612,\n",
       " 'sound': 613,\n",
       " 'arson': 614,\n",
       " 'responders': 615,\n",
       " 'nothing': 616,\n",
       " 'music': 617,\n",
       " 'cyclone': 618,\n",
       " 'remember': 619,\n",
       " 'upheaval': 620,\n",
       " 'done': 621,\n",
       " 'eyewitness': 622,\n",
       " 'electrocute': 623,\n",
       " 'thanks': 624,\n",
       " 'hours': 625,\n",
       " 'believe': 626,\n",
       " '50': 627,\n",
       " 'obliteration': 628,\n",
       " 'yet': 629,\n",
       " 'making': 630,\n",
       " 'already': 631,\n",
       " 'media': 632,\n",
       " 'days': 633,\n",
       " 'siren': 634,\n",
       " 'ur': 635,\n",
       " 'fight': 636,\n",
       " 'start': 637,\n",
       " 'few': 638,\n",
       " 'lt': 639,\n",
       " 'detonate': 640,\n",
       " 'mp': 641,\n",
       " 'shooting': 642,\n",
       " 'bioterrorism': 643,\n",
       " 'such': 644,\n",
       " 'sue': 645,\n",
       " 'legionnaires': 646,\n",
       " 'leave': 647,\n",
       " 'blight': 648,\n",
       " 'men': 649,\n",
       " 'ablaze': 650,\n",
       " 'inside': 651,\n",
       " 'fun': 652,\n",
       " 'israeli': 653,\n",
       " 'person': 654,\n",
       " 'having': 655,\n",
       " 'actually': 656,\n",
       " 'wake': 657,\n",
       " 'turkey': 658,\n",
       " 'hijack': 659,\n",
       " '16': 660,\n",
       " 'died': 661,\n",
       " '16yr': 662,\n",
       " 'policy': 663,\n",
       " 'far': 664,\n",
       " 'shot': 665,\n",
       " 'reactor': 666,\n",
       " 'both': 667,\n",
       " 'nearby': 668,\n",
       " 'bush': 669,\n",
       " 'support': 670,\n",
       " 'die': 671,\n",
       " 'lab': 672,\n",
       " 'site': 673,\n",
       " 're\\x89û': 674,\n",
       " 'stand': 675,\n",
       " 'order': 676,\n",
       " '06': 677,\n",
       " 'yes': 678,\n",
       " 'seismic': 679,\n",
       " 'trying': 680,\n",
       " 'sirens': 681,\n",
       " 'words': 682,\n",
       " 'declares': 683,\n",
       " 'health': 684,\n",
       " 'play': 685,\n",
       " 'peace': 686,\n",
       " 'pic': 687,\n",
       " 'north': 688,\n",
       " 'deluged': 689,\n",
       " 'ago': 690,\n",
       " 'wait': 691,\n",
       " 'brown': 692,\n",
       " 'horror': 693,\n",
       " \"doesn't\": 694,\n",
       " 'find': 695,\n",
       " 'islam': 696,\n",
       " 'rubble': 697,\n",
       " 'line': 698,\n",
       " 'swallowed': 699,\n",
       " 'anything': 700,\n",
       " 'nowplaying': 701,\n",
       " 'yourself': 702,\n",
       " '25': 703,\n",
       " 'low': 704,\n",
       " 'guys': 705,\n",
       " \"i'd\": 706,\n",
       " 'plans': 707,\n",
       " '\\x89ûó': 708,\n",
       " 'children': 709,\n",
       " 'gets': 710,\n",
       " '01': 711,\n",
       " 'stretcher': 712,\n",
       " 'abc': 713,\n",
       " 'snowstorm': 714,\n",
       " 'place': 715,\n",
       " 'own': 716,\n",
       " 'almost': 717,\n",
       " 'business': 718,\n",
       " 'aircraft': 719,\n",
       " 'history': 720,\n",
       " 'data': 721,\n",
       " 'tell': 722,\n",
       " 'deal': 723,\n",
       " 'photos': 724,\n",
       " 'watching': 725,\n",
       " 'yeah': 726,\n",
       " 'outside': 727,\n",
       " 'job': 728,\n",
       " 'second': 729,\n",
       " 'bc': 730,\n",
       " 'memories': 731,\n",
       " \"what's\": 732,\n",
       " 'helicopter': 733,\n",
       " 'bigger': 734,\n",
       " 'saipan': 735,\n",
       " \"'conclusively\": 736,\n",
       " \"confirmed'\": 737,\n",
       " 'pick': 738,\n",
       " 'tv': 739,\n",
       " 'center': 740,\n",
       " 'reuters': 741,\n",
       " 'lost': 742,\n",
       " \"didn't\": 743,\n",
       " 'hey': 744,\n",
       " 'control': 745,\n",
       " 'bodies': 746,\n",
       " 'searching': 747,\n",
       " 'rise': 748,\n",
       " 'blizzard': 749,\n",
       " 'makes': 750,\n",
       " 'desolate': 751,\n",
       " 'west': 752,\n",
       " 'avalanche': 753,\n",
       " 'anyone': 754,\n",
       " 'transport': 755,\n",
       " 'hell': 756,\n",
       " 'hear': 757,\n",
       " 'happy': 758,\n",
       " 'projected': 759,\n",
       " 'literally': 760,\n",
       " 'bar': 761,\n",
       " 'american': 762,\n",
       " 'book': 763,\n",
       " 'waves': 764,\n",
       " 'x': 765,\n",
       " 'hat': 766,\n",
       " 'maybe': 767,\n",
       " 'bestnaijamade': 768,\n",
       " 'probably': 769,\n",
       " 'soudelor': 770,\n",
       " 'aug': 771,\n",
       " 'space': 772,\n",
       " 'team': 773,\n",
       " 'hollywood': 774,\n",
       " 'might': 775,\n",
       " 'crews': 776,\n",
       " 'manslaughter': 777,\n",
       " 'property': 778,\n",
       " 'listen': 779,\n",
       " 'side': 780,\n",
       " 'online': 781,\n",
       " 'though': 782,\n",
       " 'move': 783,\n",
       " 'pakistan': 784,\n",
       " 'feeling': 785,\n",
       " 'amid': 786,\n",
       " 'pretty': 787,\n",
       " 'money': 788,\n",
       " 'damn': 789,\n",
       " 'street': 790,\n",
       " 'eyes': 791,\n",
       " 'rd': 792,\n",
       " 'finally': 793,\n",
       " 'signs': 794,\n",
       " 'saved': 795,\n",
       " 'trains': 796,\n",
       " 'effect': 797,\n",
       " 'morning': 798,\n",
       " 'country': 799,\n",
       " 'fast': 800,\n",
       " 'once': 801,\n",
       " 'seen': 802,\n",
       " 'hailstorm': 803,\n",
       " 'everything': 804,\n",
       " 'spot': 805,\n",
       " 'hate': 806,\n",
       " 'feared': 807,\n",
       " \"we're\": 808,\n",
       " 'myself': 809,\n",
       " 'case': 810,\n",
       " '20': 811,\n",
       " 'mom': 812,\n",
       " 'wrong': 813,\n",
       " 'child': 814,\n",
       " 'annihilation': 815,\n",
       " 'leather': 816,\n",
       " 'caught': 817,\n",
       " 'town': 818,\n",
       " 'crisis': 819,\n",
       " 'okay': 820,\n",
       " 'major': 821,\n",
       " 'image': 822,\n",
       " 'nearly': 823,\n",
       " 'refugio': 824,\n",
       " 'costlier': 825,\n",
       " '17': 826,\n",
       " 'miners': 827,\n",
       " 'arsonist': 828,\n",
       " 'youth': 829,\n",
       " 'confirmed': 830,\n",
       " 'la': 831,\n",
       " 'trench': 832,\n",
       " 'blaze': 833,\n",
       " 'sensor': 834,\n",
       " 'angry': 835,\n",
       " 'jobs': 836,\n",
       " 'called': 837,\n",
       " 'cars': 838,\n",
       " 'banned': 839,\n",
       " 'mayhem': 840,\n",
       " 'name': 841,\n",
       " 'needs': 842,\n",
       " 'lord': 843,\n",
       " 'aftershock': 844,\n",
       " 'picking': 845,\n",
       " 'usa': 846,\n",
       " 'sorry': 847,\n",
       " 'texas': 848,\n",
       " 'class': 849,\n",
       " 'guy': 850,\n",
       " 'flash': 851,\n",
       " 'fukushima': 852,\n",
       " 'course': 853,\n",
       " 'russian': 854,\n",
       " 'daily': 855,\n",
       " \"they're\": 856,\n",
       " '12': 857,\n",
       " 'giant': 858,\n",
       " 'ship': 859,\n",
       " 'ball': 860,\n",
       " 'wow': 861,\n",
       " '13': 862,\n",
       " 'crazy': 863,\n",
       " 'vehicle': 864,\n",
       " 'flag': 865,\n",
       " 'knock': 866,\n",
       " 'worst': 867,\n",
       " \"she's\": 868,\n",
       " 'meek': 869,\n",
       " 'gun': 870,\n",
       " 'haha': 871,\n",
       " 'talk': 872,\n",
       " 'huge': 873,\n",
       " 'anthrax': 874,\n",
       " 'bbc': 875,\n",
       " 'poor': 876,\n",
       " 'dont': 877,\n",
       " 'toddler': 878,\n",
       " 'link': 879,\n",
       " 'chance': 880,\n",
       " 'friends': 881,\n",
       " 'closed': 882,\n",
       " 'reason': 883,\n",
       " 'follow': 884,\n",
       " 'running': 885,\n",
       " 'entire': 886,\n",
       " 'others': 887,\n",
       " 'east': 888,\n",
       " 'united': 889,\n",
       " 'houses': 890,\n",
       " 'offensive': 891,\n",
       " 'sign': 892,\n",
       " 'gbbo': 893,\n",
       " 'omg': 894,\n",
       " 'uk': 895,\n",
       " 'win': 896,\n",
       " 'action': 897,\n",
       " 'across': 898,\n",
       " \"isn't\": 899,\n",
       " '60': 900,\n",
       " 'computers': 901,\n",
       " 'official': 902,\n",
       " 'heavy': 903,\n",
       " 'ignition': 904,\n",
       " 'alarm': 905,\n",
       " 'appears': 906,\n",
       " \"'i\": 907,\n",
       " 'cake': 908,\n",
       " 'wanted': 909,\n",
       " 'disea': 910,\n",
       " 'taken': 911,\n",
       " 'totally': 912,\n",
       " 'learn': 913,\n",
       " 'become': 914,\n",
       " 'centre': 915,\n",
       " 'eye': 916,\n",
       " 'truth': 917,\n",
       " 'cnn': 918,\n",
       " \"let's\": 919,\n",
       " \"'we're\": 920,\n",
       " 'chicago': 921,\n",
       " 'reports': 922,\n",
       " 'issued': 923,\n",
       " 'takes': 924,\n",
       " 'try': 925,\n",
       " 'public': 926,\n",
       " 'radio': 927,\n",
       " 'playing': 928,\n",
       " 'hard': 929,\n",
       " 'ladies': 930,\n",
       " 'mishaps': 931,\n",
       " 'america': 932,\n",
       " 'beach': 933,\n",
       " 'emmerdale': 934,\n",
       " 'china': 935,\n",
       " 'declaration': 936,\n",
       " 'level': 937,\n",
       " '100': 938,\n",
       " 'happened': 939,\n",
       " 'christian': 940,\n",
       " 'muslims': 941,\n",
       " 'temple': 942,\n",
       " 'mount': 943,\n",
       " 'village': 944,\n",
       " 'ca': 945,\n",
       " 'view': 946,\n",
       " 'mad': 947,\n",
       " 'alone': 948,\n",
       " 'yours': 949,\n",
       " 'flight': 950,\n",
       " 'climate': 951,\n",
       " 'favorite': 952,\n",
       " 'downtown': 953,\n",
       " 'dog': 954,\n",
       " \"ain't\": 955,\n",
       " 'virgin': 956,\n",
       " 'coaches': 957,\n",
       " 'drive': 958,\n",
       " 'instead': 959,\n",
       " 'myanmar': 960,\n",
       " 'film': 961,\n",
       " '\\x89ûï': 962,\n",
       " 'france': 963,\n",
       " 'green': 964,\n",
       " \"water'\": 965,\n",
       " 'insurance': 966,\n",
       " 'driving': 967,\n",
       " '24': 968,\n",
       " 'sea': 969,\n",
       " 'womens': 970,\n",
       " 'vs': 971,\n",
       " 'star': 972,\n",
       " 'looking': 973,\n",
       " 'pain': 974,\n",
       " 'germs': 975,\n",
       " 'large': 976,\n",
       " 'blue': 977,\n",
       " 'experts': 978,\n",
       " 'drake': 979,\n",
       " 'chile': 980,\n",
       " '18': 981,\n",
       " 'global': 982,\n",
       " 'else': 983,\n",
       " 'marks': 984,\n",
       " \"reddit's\": 985,\n",
       " 'quiz': 986,\n",
       " 'mph': 987,\n",
       " 'front': 988,\n",
       " 'ready': 989,\n",
       " 'friend': 990,\n",
       " 'till': 991,\n",
       " 'sounds': 992,\n",
       " 'miss': 993,\n",
       " 'hiring': 994,\n",
       " 'park': 995,\n",
       " 'early': 996,\n",
       " 'middle': 997,\n",
       " 'trust': 998,\n",
       " 'russia': 999,\n",
       " 'date': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras pad_sequences function is used to pad the sequences with the same length. The keras pad sequence function transforms several sequences into the numpy array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 300)           6810300   \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 100)               40100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,850,501\n",
      "Trainable params: 6,850,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 187 ms, sys: 11.9 ms, total: 199 ms\n",
      "Wall time: 80.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=max_len))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 4s 29ms/step - loss: 0.6061 - accuracy: 0.6727\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.1817 - accuracy: 0.9406\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0589 - accuracy: 0.9834\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0296 - accuracy: 0.9898\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0223 - accuracy: 0.9939\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0183 - accuracy: 0.9939\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0171 - accuracy: 0.9941\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0137 - accuracy: 0.9949\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 0.0125 - accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0105 - accuracy: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4a98b53f0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=10, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is almost 1, which means that we are overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc: 0.83%\n"
     ]
    }
   ],
   "source": [
    "print(\"Auc: %.2f%%\" % (roc_auc(scores, yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'SimpleRNN','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3597,\n",
       "  203,\n",
       "  345,\n",
       "  799,\n",
       "  3598,\n",
       "  2453,\n",
       "  6,\n",
       "  3599,\n",
       "  1159,\n",
       "  714,\n",
       "  3600,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  6954,\n",
       "  3601,\n",
       "  3602]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq[:1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "word embeddings are type of word representation that allows words with similar meaning to have a similar representation.\n",
    "word embedding are in fact a class of techniques where individual words are represented by a real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172027it [00:06, 24906.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     values \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     word \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     coefs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray([\u001b[39mfloat\u001b[39;49m(val) \u001b[39mfor\u001b[39;49;00m val \u001b[39min\u001b[39;49;00m values[\u001b[39m1\u001b[39;49m:]])\n\u001b[1;32m      9\u001b[0m     embeddings_index[word] \u001b[39m=\u001b[39m coefs\n\u001b[1;32m     10\u001b[0m f\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('Data/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2 LSTM\n",
    "\n",
    "Long-Short Term Memory networks or LSTM are a variant of RNN that solve the long term memory problem of the former.\n",
    "\n",
    "they have a more complex cell structure than a normal recurrent neuron, that allows them to better regulate how to learn or forget efficiently from the diffrent input sources.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22700/22700 [00:00<00:00, 475746.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\\\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    # A simple LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Building the embedding layer and passing the created embedding matrix as weights to the layer instead of training it over vocabulary\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 0.0116 - accuracy: 0.9949\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0110 - accuracy: 0.9951\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0100 - accuracy: 0.9961\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0099 - accuracy: 0.9959\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 3s 29ms/step - loss: 0.0088 - accuracy: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4a9b9cb50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 4ms/step\n",
      "Auc: 0.79%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see the auc is 0.88, which has increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 4ms/step\n",
      "Auc: 0.79%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = pd.read_csv('Data/sample_submission.csv')\n",
    "#test_dat = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model.predict(test_dat)\n",
    "# sub.to_csv('submission.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
